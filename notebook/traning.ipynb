{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model identify if two images are the same turtle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from os.path import join\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "from ShellRec.data_utils.prepare_photos import get_img_graph, split_graph\n",
    "from ShellRec.training_utils.shell_rec import TurtleDiff, TurtleDataset, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing photo files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train = get_img_graph(path = \"../dataset/BoxTurtles\")\n",
    "split_graph(all_train, save_path = \"../dataset\")\n",
    "_ = get_img_graph(path = \"../dataset/BoxTurtlesHoldout\", \n",
    "                  file_to_save = \"../dataset/shell_rec_holdout.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## image transformations\n",
    "transform_train = transforms.create_transform(384, is_training = True, \n",
    "                                   auto_augment = \"rand-m9-mstd0.5\")\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.4850, 0.4560, 0.4060], \n",
    "                             std=[0.2290, 0.2240, 0.2250])\n",
    "])\n",
    "    \n",
    "# Set up datasets and dataloaders\n",
    "train_loader = DataLoader( TurtleDataset(data_file='../dataset/train.json', \n",
    "                                             transform=transform_train), \n",
    "                                             batch_size = 20)\n",
    "val_loader = DataLoader( TurtleDataset(data_file='../dataset/val.json', \n",
    "                                           transform=transform_test), \n",
    "                                           batch_size = 20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model set up, use a pretrained model as backbone, and add a new head to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')   \n",
    "\n",
    "model = TurtleDiff('vit_base_patch16_384') # use a vit backbone\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, optimizer, criterion, train_loader, val_loader, device, num_epochs,\n",
    "      save_path = \"../pretrained\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
